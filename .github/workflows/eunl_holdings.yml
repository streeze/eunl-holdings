name: EUNL Yearly Holdings

"on":
  workflow_dispatch:

permissions:
  contents: write

jobs:
  build:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Setup Python 3.8 (legacy wheels)
        uses: actions/setup-python@v5
        with:
          python-version: "3.8"

      - name: Upgrade pip & core build tools
        run: |
          python -m pip install --upgrade pip
          pip install --upgrade wheel "setuptools<81"

      - name: Install system deps (lxml)
        run: |
          sudo apt-get update
          sudo apt-get install -y libxml2-dev libxslt1-dev zlib1g-dev

      - name: Clone talsan/ishares (scraper)
        run: git clone --depth 1 https://github.com/talsan/ishares.git vendor/ishares

      - name: Install ishares requirements (pinned, legacy)
        run: |
          pip install -r vendor/ishares/requirements.txt

      - name: Install extra Python deps (runtime needs)
        run: |
          pip install \
            pandas pyarrow requests python-dateutil beautifulsoup4 html5lib \
            scrapingbee cloudscraper pandas_market_calendars backports.zoneinfo \
            openpyxl xlrd \
            "urllib3<2" "certifi<2021" "chardet<4" "idna<3"

      - name: Show environment (debug)
        run: |
          python -V
          pip --version
          pip list | sort | tee pip-freeze.txt
          echo "Workspace:"
          pwd && ls -la
          echo "ishares tree:"
          ls -la vendor/ishares || true
          ls -la vendor/ishares/ishares || true

      # --- Downloader im richtigen Verzeichnis starten
      - name: Run downloader with retries
        id: download
        working-directory: vendor/ishares
        env:
          PYTHONPATH: .
          # Optional: eigenes Secret nutzen
          # SCRAPINGBEE_API_KEY: ${{ secrets.SCRAPINGBEE_API_KEY }}
        run: |
          set -e
          tries=3
          for i in $(seq 1 $tries); do
            echo "Attempt $i/$tries"
            python -m ishares.sync_etf_downloader --outputpath ../../data/ishares SWDA --overwrite && break
            echo "Downloader failed (attempt $i). Sleeping 10s..."
            sleep 10
          done

      - name: List downloaded files (debug)
        run: |
          echo "Listing data/ishares..."
          find data/ishares -maxdepth 6 -type f | head -n 200 || true
          echo "Count of CSVs:"
          find data/ishares -name "asofdate=*.csv" | wc -l || true

      # --- Skript AUTOMATISCH schreiben, falls nicht vorhanden
      - name: Write select_yearly_snapshots.py (self-heal)
        run: |
          mkdir -p scripts
          cat > scripts/select_yearly_snapshots.py <<'PY'
#!/usr/bin/env python3
import argparse
import sys
from pathlib import Path
import pandas as pd
from dateutil.parser import isoparse

# Input directory structure produced by talsan/ishares downloader:
# {root}/type=holdings/state=formatted/etf=SWDA/asofdate=YYYY-MM-DD.csv

def find_csvs(root: Path, etf: str):
    patt = root / 'type=holdings' / 'state=formatted' / f'etf={etf}'
    if not patt.exists():
        return []
    return sorted(patt.glob('asofdate=*.csv'))

def parse_asofdate(p: Path):
    # filename: asofdate=YYYY-MM-DD.csv
    name = p.stem  # asofdate=YYYY-MM-DD
    try:
        date_str = name.split('=')[1]
        dt = isoparse(date_str).date()
        return dt
    except Exception:
        return None

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument('--root', required=True, help='Root path where ishares downloader wrote files (e.g., data/ishares)')
    ap.add_argument('--etf', default='SWDA', help='ETF code inside ishares dataset (SWDA for EUNL/SWDA)')
    ap.add_argument('--from-year', type=int, default=2010)
    ap.add_argument('--outdir', default='exports/eunl_yearly')
    args = ap.parse_args()

    root = Path(args.root)
    outdir = Path(args.outdir)
    outdir.mkdir(parents=True, exist_ok=True)
    summary_dir = Path('summary')
    summary_dir.mkdir(parents=True, exist_ok=True)

    files = find_csvs(root, args.etf)
    if not files:
        print('No CSVs found. Did the downloader run successfully?', file=sys.stderr)
        sys.exit(1)

    by_year = {}
    for p in files:
        dt = parse_asofdate(p)
        if not dt:
            continue
        if dt.year < args.from_year:
            continue
        by_year.setdefault(dt.year, []).append((dt, p))

    rows = []
    for year, items in sorted(by_year.items()):
        items.sort(key=lambda x: x[0])
        dt, p = items[-1]
        df = pd.read_csv(p)

        weight_cols = [c for c in df.columns if c.lower() in {'weight', 'weight_percent', 'weighting', 'weight (%)', 'fund weight'}]
        weight_sum = None
        if weight_cols:
            try:
                weight_sum = pd.to_numeric(df[weight_cols[0]], errors='coerce').sum()
            except Exception:
                weight_sum = None

        n_missing_isin = df['isin'].isna().sum() if 'isin' in df.columns else None
        n_rows = len(df)

        out_name = outdir / f"EUNL_holdings_{dt.isoformat()}.csv"
        df.to_csv(out_name, index=False)

        rows.append({
            'year': year,
            'asofdate': dt.isoformat(),
            'export_file': str(out_name),
            'n_holdings': n_rows,
            'weight_sum_raw': weight_sum,
            'missing_isin': n_missing_isin,
            'source_csv': str(p)
        })

    summary = pd.DataFrame(rows).sort_values(['year'])
    summary_path = summary_dir / 'EUNL_yearly_summary.csv'
    summary.to_csv(summary_path, index=False)
    print(f"Wrote {len(summary)} yearly files to {outdir}")
    print(f"Summary: {summary_path}")

if __name__ == '__main__':
    main()
PY
          chmod +x scripts/select_yearly_snapshots.py
          echo "Wrote scripts/select_yearly_snapshots.py"

      - name: Select latest snapshot per year (>= 2010)
        run: |
          python scripts/select_yearly_snapshots.py \
            --root data/ishares \
            --etf SWDA \
            --from-year 2010 \
            --outdir exports/eunl_yearly

      - name: List exports (debug)
        run: |
          echo "Summary dir:"
          ls -la summary || true
          echo "Exports dir:"
          ls -la exports/eunl_yearly || true

      - name: Upload exports artifact (always try)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: eunl_yearly_exports
          path: |
            exports/eunl_yearly
            summary
            pip-freeze.txt
          if-no-files-found: warn

      - name: Commit & push results back to repo
        run: |
          git config user.name "github-actions"
          git config user.email "github-actions@users.noreply.github.com"
          git add exports/eunl_yearly summary/* || true
          if ! git diff --cached --quiet; then
            git commit -m "Update EUNL yearly holdings exports"
            git push
          else
            echo "No changes to commit"
          fi
